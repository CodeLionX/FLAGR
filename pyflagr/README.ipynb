{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bfd6be",
   "metadata": {},
   "source": [
    "# (Py)FLAGR\n",
    "\n",
    "Fuse, Learn, AGgregate, Rerank\n",
    "\n",
    "FLAGR is a high performing, modular library for rank aggregation. To ensure the highest possible performance, the core FLAGR library is written in C++ and implements a wide collection of unsupervised rank aggregation methods. Its modular design allows third-party programmers to implement their own algorithms and easily rebuild the entire library. FLAGR can be built as a standard application, or as a shared library (`so` or `dll`). In the second case, it can be linked from other C/C++ programs, or even from programs written in other languages (e.g. Python, PHP, etc.).\n",
    "\n",
    "In this context, PyFLAGR is a Python library that links to FLAGR and allows a developer to exploit the efficient FLAGR implementations from a standard Python program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6962d8d",
   "metadata": {},
   "source": [
    "## Compiling FLAGR as a shared library\n",
    "\n",
    "The FLAGR shared library has been already pre-built and tested with the GCC compiler. The FLAGR Github repository includes the appropriate `.so` and `.dll` dynamic libraries in the `pyflagr` directory, so PyFLAGR can be immediately installed without compilation.\n",
    "\n",
    "Nevertheless, in case a custom rank aggregation method has been implemented with FLAGR, or any modification in the C++ code has been made, FLAGR must be rebuilt and PyFLAGR must be reinstalled. For Linux-based systems with the GCC compiler, FLAGR can be built as a shared library by invoking the following system command:\n",
    "\n",
    "`g++ -O3 -Wall -Werror -shared -std=c++11 -fPIC /home/leo/Desktop/code/FLAGR/cflagr.cpp -o /home/leo/Desktop/code/FLAGR/pyflagr/flagr.so`\n",
    "\n",
    "This command will generate the necessary `.so` library.\n",
    "\n",
    "For Windows-based systems with the GCC compiler, FLAGR can be built as a Dynamic Link Library by invoking the following system commands:\n",
    "\n",
    "* `g++ -O3 -c -o flagr.o C:/Users/Leo/Documents/cpp_algorithms/FLAGR/cflagr.cpp`\n",
    "* `g++ -O3 -o C:/Users/Leo/Documents/cpp_algorithms/FLAGR/pyflagr/pyflagr/flagr.dll -s -shared flagr.o -Wl,--subsystem,windows`\n",
    "\n",
    "- OR -\n",
    "* `g++ -O3 -c -o /home/leo/Desktop/code/FLAGR/pyflagr/pyflagr/flagr.o /home/leo/Desktop/code/FLAGR/cflagr.cpp`\n",
    "* ``\n",
    "This command will generate the necessary `.dll` library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010100c",
   "metadata": {},
   "source": [
    "## Installing PyFLAGR\n",
    "\n",
    "The installation of PyFLAGR can be performed by employing the `pip` Python package installer. Initially we need to navigate to the directory where the `setup.py` file resides.\n",
    "\n",
    "In the current Linux Mint installation the directory is `cd /home/leo/Desktop/code/FLAGR/pyflagr/`\n",
    "\n",
    "In the current Windows7 installation the directory is `cd C:\\Users\\Leo\\Documents\\cpp_algorithms\\FLAGR\\pyflagr`\n",
    "\n",
    "After navigating to the directory where `setup.py` resides, install it by using `pip`:\n",
    "\n",
    "`pip install .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346fa4d",
   "metadata": {},
   "source": [
    "## Importing and using PyFLAGR\n",
    "\n",
    "PyFLAGR groups its supported rank aggregation methods in four modules:\n",
    "\n",
    "1. `Comb`: In this module the `CombSUM` and `CombMNZ` methods are implemented. Each method comes in four variants according to the rank/score normalization method. Future releases of FLAGR will also include CombAVG, CombMAX and CombMIN.\n",
    "2. `Majoritarian`: Includes `CondorcetWinners` and `OutrankingApproach`.\n",
    "3. `MarkovChains`: The fourth and most popular method (termed `MC4`) based on Markov Chains is implemented. Future releases of FLAGR will include the other three implementations.\n",
    "4. `Weighted`: This module implements several self-weighting rank aggregation methods. These methods automatically identify the expert voters and include:\n",
    " 1. The Preference Relations Graph method of .\n",
    " 2. The Agglomerative method of .\n",
    " 3. The Iterative, Distance-Based method of .\n",
    "\n",
    "The following statements demonstrate the imports of all PyFLAGR rank aggregation methods in a typical jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57901c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflagr.Comb as SCORE_BASED\n",
    "import pyflagr.Majoritarian as ORDER_BASED\n",
    "import pyflagr.MarkovChains as MARKOV_CHAINS\n",
    "import pyflagr.Weighted as WGT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f16813",
   "metadata": {},
   "source": [
    "All PyFLAGR rank aggregation methods include:\n",
    "* a standard class constructor: several hyper-parameters of the corresponding algorithm  and other execution arguments can be passed through the constructor. All the constructor inputs have default values, therefore, they are considered optional. This means that all constructors can be called *any* argument at all.\n",
    "* an `aggregate` method that runs the algorithm on the selected input and (optionally) evaluates the generated aggregate list. In all algorithms, `aggregate` method accepts the following arguments:\n",
    "\n",
    "| Parameter    | Type                                         | Default Value  | Values  |\n",
    "| :----------- | :--------------------------------------------| :--------------| :------ |\n",
    "| `input_file` | String - Required, unless `input_df` is set. | Empty String   | A CSV file that contains the input lists to be aggregated. |\n",
    "| `input_df` | Pandas DataFrame - Required, unless `input_file` is set. | `None` | A Pandas DataFrame that contains the input lists to be aggregated. **Note:** If both `input_file` and `input_df` are set, only the former is used; the latter is ignored. |\n",
    "| `rels_file`  | String, Optional. | Empty String | A CSV file that contains the relevance judgements of the involved list elements. If such a file is passed, FLAGR will evaluate the generated aggregate list/s by computing several retrieval effectiveness evaluation measures. The results of the evaluation will be stored in the `eval_df` DataFrame. Otherwise, no evaluation will take place and `eval_df` will be empty. Read more on the evaluation of rank aggregation quality. |\n",
    "| `rels_df`    | Pandas DataFrame, Optional. | `None` | A Pandas DataFrame that contains the relevance judgements of the involved list elements. If such a dataframe is passed, FLAGR will evaluate the generated aggregate list/s by computing several retrieval effectiveness evaluation measures. The results of the evaluation will be stored in the `eval_df` DataFrame. Otherwise, no evaluation will take place and `eval_df` will be empty. Read more on the evaluation of rank aggregation quality. **Note:** If both `rels_file` and `rels_df` are set, only the former is used; the latter is ignored. |\n",
    "| `output_dir` | String, Optional. | Temporary directory (OS-specific) | The directory where the output files (aggregate lists and evaluation) will be stored. If it is not set, the default location will be used. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec76d8d",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "The core library, FLAGR, accepts data (namely, the input lists to be aggregated) in a single, specially formatted CSV file. The columns in the CSV file are organized according to the following manner:\n",
    "\n",
    "`Query/Topic,Voter,Item,Score,Algorithm/Dataset`\n",
    "\n",
    "where:\n",
    "* `Query/Topic`: the query string or the topic for which the list is submitted.\n",
    "* `Voter`: the name of the voter, or the ranker who submits the list.\n",
    "* `Item`: a unique name that identifies a particular element in the list. A voter cannot submit the same element for the same query/topic two or more times. This means that each element appears exactly once in each list. However, the same element may appear in lists submitted by other voters.\n",
    "* `Score`: the score assigned to an `Item` by a specific `Voter`. In may cases (e.g. search engine rankings), the individual scores are unknown. In such cases the scores can be replaced by the (reverse) ranking of an `Item` in such a manner that the top rankings receive higher scores than the ones that have been assigned lower rankings.\n",
    "\n",
    "PyFLAGR has two mechanisms for passing data to FLAGR, namely:\n",
    "* either by forwarding the name and the location of the aforementioned input CSV file (this is the `input_file` argument of the `aggregate` method),\n",
    "* or by accepting a Pandas Dataframe from the user (this is the `input_df` argument of the `aggregate` method). In this case, PyFLAGR internally dumps the `input_df` contents into a temporary CSV file and passes the name and the location of that temporary file to FLAGR.\n",
    "\n",
    "\n",
    "Optionally, the user may specify a second CSV file (called as `rels_file`), or a Dataframe (called as `rels_df`) that contain judgments about the relevance of the included elements w.r.t a query. The columns in `rels_file` are organized as follows:\n",
    "\n",
    "`Query/Topic,0,Item,Relevance`\n",
    "\n",
    "where:\n",
    "* `Query/Topic`: the query string or the topic for which the corresponding `Item` is evaluated.\n",
    "* `0`: A hypothetical hyper-voter (also called voter `0`) who has flawless knowledge of the `Query/Topic` and determines whether an `Item` is relevant to it, or not. The value of this column must be always `0`.\n",
    "* `Item`: a unique name that identifies a particular element of which the relevance to the `Query/Topic` is evaluated.\n",
    "* `Relevance`: the relevance score assigned to an `Item` by `Voter 0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359484b",
   "metadata": {},
   "source": [
    "## Output data format\n",
    "\n",
    "In all cases the core library, FLAGR, writes the generated aggregate list in a plain CSV file. PyFLAGR reads the contents of this CSV file into a Pandas Dataframe which is returned to the user.\n",
    "\n",
    "Optionally, FLAGR may also create a second output file to write the results of the evaluation of the effectiveness of an algorithm. This happens when a `rels_file` is provided to the algorithm. The `aggregate` method of all algorithms *always* returns two Pandas Dataframes according to the provided input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ac1d6",
   "metadata": {},
   "source": [
    "## Code examples\n",
    "\n",
    "The following examples demonstrate the usage of all PyFLAGR rank aggregation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed56b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists = '/media/leo/B65266EC5266B1331/phd_Research/08 - Datasets/TREC/Synthetic/MOSO.csv'\n",
    "#qrels = '/media/leo/B65266EC5266B1331/phd_Research/08 - Datasets/TREC/Synthetic/MOSO_qrels.csv'\n",
    "\n",
    "lists = 'D:/phd_Research/08 - Datasets/TREC/Synthetic/MOSO.csv'\n",
    "qrels = 'D:/phd_Research/08 - Datasets/TREC/Synthetic/MOSO_qrels.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb444",
   "metadata": {},
   "source": [
    "### Comb methods: CombSUM\n",
    "\n",
    "Member of `pyflagr.Comb`.\n",
    "\n",
    "The `CombSUM` constructor supports the following parameters:\n",
    "\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `norm` | String, Optional. | `borda` | Rank or score normalization methods:<ul><li>`borda`: The aggregation is performed by normalizing the element *rankings* according to the Borda normalization method. Equivalent to the `BordaCount` function.</li><li>`rank`: The aggregation is performed by normalizing the element *rankings* according to the Rank normalization method.</li><li>`score`: The aggregation is performed by normalizing the element *scores* according to the Score normalization method.</li><li>`z-score`: The aggregation is performed by normalizing the element *scores* according to the Z-Score normalization method.</li></ul> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2fb521",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\Users\\Leo\\Documents\\cpp_algorithms\\FLAGR\\pyflagr\\pyflagr\\flagr.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m csum \u001b[38;5;241m=\u001b[39m \u001b[43mSCORE_BASED\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCombSUM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# In this case, rels_file has been specified, so PyFLAGR returns two non-blank dataframes:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# * df_out contains the aggregate list produced by the selected algorithm\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# * df_eval contains the effectiveness evaluation based on the relevance judgments in the rels_file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_out, df_eval \u001b[38;5;241m=\u001b[39m csum\u001b[38;5;241m.\u001b[39maggregate(input_file\u001b[38;5;241m=\u001b[39mlists, rels_file\u001b[38;5;241m=\u001b[39mqrels)\n",
      "File \u001b[1;32m~\\Documents\\cpp_algorithms\\FLAGR\\pyflagr\\pyflagr\\Comb.py:11\u001b[0m, in \u001b[0;36mCombSUM.__init__\u001b[1;34m(self, norm, eval_pts)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mborda\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_pts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mRAM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_pts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization \u001b[38;5;241m=\u001b[39m norm\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagr_lib\u001b[38;5;241m.\u001b[39mComb\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_char_p,  \u001b[38;5;66;03m# Input data file with the lists to be aggregated\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_char_p,  \u001b[38;5;66;03m# Input data file with the relevant elements per query - used for evaluation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_char_p,  \u001b[38;5;66;03m# Random string to be embedded into the output file names\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_char_p]\n",
      "File \u001b[1;32m~\\Documents\\cpp_algorithms\\FLAGR\\pyflagr\\pyflagr\\RAM.py:33\u001b[0m, in \u001b[0;36mRAM.__init__\u001b[1;34m(self, eval_pts)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagr_lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18m__file__\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/flagr.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m platform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagr_lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/flagr.dll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m platform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagr_lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18m__file__\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/flagr.dylib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ctypes\\__init__.py:382\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Users\\Leo\\Documents\\cpp_algorithms\\FLAGR\\pyflagr\\pyflagr\\flagr.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "csum = SCORE_BASED.CombSUM(norm='rank')\n",
    "\n",
    "# In this case, rels_file has been specified, so PyFLAGR returns two non-blank dataframes:\n",
    "# * df_out contains the aggregate list produced by the selected algorithm\n",
    "# * df_eval contains the effectiveness evaluation based on the relevance judgments in the rels_file\n",
    "df_out, df_eval = csum.aggregate(input_file=lists, rels_file=qrels)\n",
    "\n",
    "df_out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e23566",
   "metadata": {},
   "outputs": [],
   "source": [
    "csum = SCORE_BASED.CombSUM(norm='rank')\n",
    "\n",
    "# In this case, rels_file has NOT been specified, so PyFLAGR returns two dataframes,\n",
    "# * df_out contains the aggregate list produced by the selected algorithm\n",
    "# * df_eval is blank\n",
    "df_out, df_eval = csum.aggregate(input_file=lists)\n",
    "\n",
    "df_out.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02eb5e8",
   "metadata": {},
   "source": [
    "### Comb methods: BordaCount\n",
    "\n",
    "Member of `pyflagr.Comb`.\n",
    "\n",
    "`BordaCount` is equivalent to `CombSUM` with `borda` normalization. Its constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "borda = SCORE_BASED.BordaCount()\n",
    "\n",
    "df_out, df_eval = borda.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent code for Borda Count: This one produces the same results as the previous code block\n",
    "csum = SCORE_BASED.CombSUM(norm='borda')\n",
    "\n",
    "df_out, df_eval = csum.aggregate(input_file=lists, rels_file=qrels)\n",
    "\n",
    "df_eval.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23031ee",
   "metadata": {},
   "source": [
    "### Comb methods: CombMNZ\n",
    "\n",
    "Member of `pyflagr.Comb`.\n",
    "\n",
    "The `CombMNZ` constructor supports the following parameters:\n",
    "\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `norm` | String, Optional. | `borda` | Rank or score normalization methods:<ul><li>`borda`: The aggregation is performed by normalizing the element *rankings* according to the Borda normalization method. Equivalent to the `BordaCount` function.</li><li>`rank`: The aggregation is performed by normalizing the element *rankings* according to the Rank normalization method.</li><li>`score`: The aggregation is performed by normalizing the element *scores* according to the Score normalization method.</li><li>`z-score`: The aggregation is performed by normalizing the element *scores* according to the Z-Score normalization method.</li></ul> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b986726",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnz = SCORE_BASED.CombMNZ(norm='rank')\n",
    "\n",
    "df_out, df_eval = cmnz.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba5e40",
   "metadata": {},
   "source": [
    "### Majoritarian methods: Concorcet Winners\n",
    "\n",
    "Member of `pyflagr.Majoritarian`.\n",
    "\n",
    "The `CondorcetWinners` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "condorcet = ORDER_BASED.CondorcetWinners()\n",
    "\n",
    "df_out, df_eval = condorcet.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514a31a",
   "metadata": {},
   "source": [
    "### Majoritarian methods: Copeland Winners\n",
    "\n",
    "Member of `pyflagr.Majoritarian`.\n",
    "\n",
    "The `CopelandWinners` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copeland = ORDER_BASED.CopelandWinners()\n",
    "\n",
    "df_out, df_eval = copeland.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08211e81",
   "metadata": {},
   "source": [
    "### Majoritarian methods: Outranking Approach\n",
    "\n",
    "Member of `pyflagr.Majoritarian`.\n",
    "\n",
    "The `OutrankingApproach` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `pref` | Hyperparameter, Float, Optional. | 0    | Preference threshold.  |\n",
    "| `veto` | Hyperparameter, Float, Optional. | 0.75 | Veto threshold.        |\n",
    "| `conc` | Hyperparameter, Float, Optional. | 0    | Concordance threshold. |\n",
    "| `disc` | Hyperparameter, Float, Optional. | 0.25 | Discordance threshold. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outrank = ORDER_BASED.OutrankingApproach()\n",
    "\n",
    "df_out, df_eval = outrank.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe1b2b",
   "metadata": {},
   "source": [
    "### Markov Chain methods: MC4\n",
    "\n",
    "Member of `pyflagr.MarkovChains`.\n",
    "\n",
    "The `MC4` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `ergodic_number` | Hyperparameter, Float, Optional. | 0.15    | The ergodic number.  |\n",
    "| `delta` | Hyperparameter, Float, Optional. | 0.00000001 | The $\\delta$ hyperparameter.        |\n",
    "| `max_iterations` | Hyperparameter, Integer, Optional. | 200    | Maximum number of iterations. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mch4 = MARKOV_CHAINS.MC4()\n",
    "\n",
    "df_out, df_eval = mch4.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e9060",
   "metadata": {},
   "source": [
    "### Weighted methods: Preferelence Relations Graph\n",
    "\n",
    "Member of `pyflagr.Weighted`.\n",
    "\n",
    "The `PreferenceRelationsGraph` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `alpha`| Hyperparameter, Float, Optional. | 0.1 | The $\\alpha$ hyper-parameter of the algorithm.  |\n",
    "| `beta` | Hyperparameter, Float, Optional. | 0.5 | The $\\beta$ hyper-parameter of the algorithm.  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90965277",
   "metadata": {},
   "outputs": [],
   "source": [
    "prf_graph = WGT.PreferenceRelationsGraph(alpha=0.1, beta=0.5)\n",
    "\n",
    "df_out, df_eval = prf_graph.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c0174",
   "metadata": {},
   "source": [
    "### Weighted methods: Agglomerative Aggregation\n",
    "\n",
    "Member of `pyflagr.Weighted`.\n",
    "\n",
    "The `Agglomerative` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `c1` | Hyperparameter, Float, Optional. | 2.5 | The $c_1$ hyper-parameter of the algorithm.  |\n",
    "| `c2` | Hyperparameter, Float, Optional. | 1.5 | The $c_2$ hyper-parameter of the algorithm.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = WGT.Agglomerative(c1=0.1, c2=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5ee8c",
   "metadata": {},
   "source": [
    "### Weighted methods: Iterative Distance-Based Aggregation\n",
    "\n",
    "Member of `pyflagr.Weighted`.\n",
    "\n",
    "The `DIBRA` constructor supports the following parameters:\n",
    "\n",
    "| Parameter      | Type        | Default Value  | Values |\n",
    "| :------------- | :---------- | :--------------| :------|\n",
    "| `eval_pts` | Integer, Optional. Considered only if `rels_file` or `rels_df` is set. | 10 | Determines the elements in the aggregate list on which the evaluation measures (i.e. Precision, and nDCG) will be computed. For example, for `eval_pts=10` FLAGR will compute $P@1, P@2, ... P@10$, and $N@1, N@2, ..., N@10$. |\n",
    "| `aggregator` | Hyperparameter, String, Optional. | `combsum:borda` | The baseline aggregation method. An extended weighted variant of the baseline method is applied internally by plugging the computed voter weights.<br> The list of the supported values includes:<br><ul><li>`combsum:borda`: CombSUM with Borda rank normalization.</li><li>`combsum:rank`: CombSUM with rankings normalization.</li><li>`combsum:score`: CombSUM with score min-max normalization.</li><li>`combsum:z-score`: CombSUM with score z-normalization.</li><li>`combmnz:borda`: CombMNZ with Borda rank normalization.</li><li>`combmnz:rank`: CombMNZ with rankings normalization.</li><li>`combmnz:score`: CombMNZ with score min-max normalization.</li><li>`combmnz:z-score`: CombMNZ with score z-normalization.</li><li>`condorcet`: The Condorcet Winners method.</li><li>`outrank`: The Outranking Approach.</li></ul>|\n",
    "| `w_norm` | Hyperparameter, String, Optional. | `minmax` | The voter weights normalization method. The list of the supported values includes:<br><ul><li>`none`: The voter weights will not be normalized.</li><li>`minmax`: The voter weights will be normalized with min-max scaling.</li><li>`z`: The voter weights will be z-normalized</li></ul> |\n",
    "| `dist` | Hyperparameter, String, Optional. | `cosine` | The metric that is used to measure the distance between an input list and the temporary aggregate list. The list of the supported values includes:<br><ul><li>`rho`: The Spearman's $\\rho$ correlation coefficient.</li><li>`cosine`: Cosine similarity of the lists' vector representations.</li><li>`tau`: The Kendall's $\\tau$ correlation coefficient.</li><li>`footrule`: A scaled variant of Spearman's Footrule distance.</li></ul> |\n",
    "| `gamma` | Hyperparameter, Float, Optional. | 1.50 | Regulates the weight convergence speed. |\n",
    "| `prune` | Hyperparameter, Boolean, Optional. | `False` | Triggers a weight-dependant list pruning mechanism. |\n",
    "| `d1`    | Hyperparameter, Float, Optional. Used only when `prune=True` | 0.4 | The hyperparameter $\\delta_1$ of the weight-dependant list pruning mechanism. |\n",
    "| `d2`    | Hyperparameter, Float, Optional. Used only when `prune=True` | 0.1 | The hyperparameter $\\delta_2$ of the weight-dependant list pruning mechanism. |\n",
    "| `tol`    | Hyperparameter, Float, Optional. | 0.01 | Controls the convergence precision. This tolerance threshold represents the minimum precision of the difference between the voter weight in an iteration and the voter weight of the previous iteration.|\n",
    "| `max_iter` | Hyperparameter, Integer, Optional. | 50 | Controls the maximum number of iterations. FLAGR will stop the execution of DIBRA if the requested number of iterations have been performed, even if the voter weights have not fully converged.|\n",
    "| `pref` | Hyperparameter, Float, Optional. | 0    | Preference threshold.  |\n",
    "| `veto` | Hyperparameter, Float, Optional. | 0.75 | Veto threshold.        |\n",
    "| `conc` | Hyperparameter, Float, Optional. | 0    | Concordance threshold. |\n",
    "| `disc` | Hyperparameter, Float, Optional. | 0.25 | Discordance threshold. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_1 = WGT.DIBRA()\n",
    "\n",
    "df_out, df_eval = method_1.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8186d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_2 = WGT.DIBRA(eval_pts=20, gamma=1.5, prune=True, d1=0.3, d2=0.05)\n",
    "\n",
    "df_out, df_eval = method_2.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_3 = WGT.DIBRA(eval_pts=20, aggregator=\"outrank\")\n",
    "\n",
    "df_out, df_eval = method_3.aggregate(input_file=lists, rels_file=qrels)\n",
    "df_eval.tail(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
